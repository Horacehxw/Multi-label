{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMap + BIHT(Binary Interactive Hard Thresholding)\n",
    "We are now recovering the BMap(1bit compressive sensing) using BIHT instead of lazy kNN.\n",
    "\n",
    "https://arxiv.org/pdf/1104.3160.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import data_util\n",
    "import BMapModel\n",
    "#from data_util import DataPoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import faiss\n",
    "# import joblib # version incompatibel with sklearn's joblib and can't load the previous model\n",
    "\n",
    "from util import map_2_z\n",
    "from util import precision_at_k\n",
    "from util import sign\n",
    "from sklearn.externals import joblib # store classifiers\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # convert y to {0,1}^L\n",
    "from sklearn.preprocessing import StandardScaler # normalize features \n",
    "from sklearn.feature_extraction import DictVectorizer # extract feature vector to x\n",
    "from numpy.random import normal # generate transforming matrix\n",
    "from sklearn.neighbors import KDTree #KDTree for fast kNN search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from joblib import Parallel, delayed # Multitread\n",
    "from pytictoc import TicToc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data:\r\n",
      "AmazonCat  Delicious  Mediamill  README_Datasets  XMLDatasetRead\r\n",
      "Bibtex\t   Eurlex     RCV1-x\t Wiki10\r\n",
      "\r\n",
      "../data/AmazonCat:\r\n",
      "amazonCat_test.txt  amazonCat_train.txt\r\n",
      "\r\n",
      "../data/Bibtex:\r\n",
      "Bibtex_data.txt  bibtex_trSplit.txt  bibtex_tstSplit.txt\r\n",
      "\r\n",
      "../data/Delicious:\r\n",
      "Delicious_data.txt  delicious_trSplit.txt  delicious_tstSplit.txt\r\n",
      "\r\n",
      "../data/Eurlex:\r\n",
      "eurlex_test.txt  eurlex_train.txt\r\n",
      "\r\n",
      "../data/Mediamill:\r\n",
      "Mediamill_data.txt  mediamill_trSplit.txt  mediamill_tstSplit.txt\r\n",
      "\r\n",
      "../data/RCV1-x:\r\n",
      "rcv1x_test.txt\trcv1x_train.txt\r\n",
      "\r\n",
      "../data/Wiki10:\r\n",
      "wiki10_test.txt  wiki10_train.txt\r\n",
      "\r\n",
      "../data/XMLDatasetRead:\r\n",
      "XMLDatasetRead\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead:\r\n",
      "ReadData_Matlab  README_Datasets\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead/ReadData_Matlab:\r\n",
      "make.m\tread_data.cpp  README.txt  write_data.cpp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m../data/Delicious/Delicious_data.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/Delicious/Delicious_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "model_dir = \"../.model2\"\n",
    "train_filename = \"/Delicious/Delicious_data.txt\"\n",
    "#test_filename = \"/Eurlex/eurlex_test.txt\"\n",
    "tr_split_file = \"/Delicious/delicious_trSplit.txt\"\n",
    "te_split_file = \"/Delicious/delicious_tstSplit.txt\"\n",
    "\n",
    "path = os.path.dirname(train_filename)\n",
    "model_path = model_dir + path\n",
    "num_core = 8\n",
    "L_hat_ratio = 0.5\n",
    "time = TicToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_point=16105, num_label=983, num_feature=500\n"
     ]
    }
   ],
   "source": [
    "tr_data, num_point, num_feature, num_label = data_util.read_file(data_dir+train_filename)\n",
    "print(\"num_point={}, num_label={}, num_feature={}\".format(num_point, num_label, num_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#te_data, _, _, _ = data_util.read_file(data_dir+test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_split = data_util.split_data(data=tr_data, split_file=data_dir+tr_split_file)\n",
    "te_split = data_util.split_data(data=tr_data, split_file=data_dir+te_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_te, Y_te = data_util.data_transform(tr_split, te_split, num_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary mapping + BIHT\n",
    "data set $(x,y)^d$, where $y_i=\\{1, 0\\}^L$\n",
    "\n",
    "we want to map y into lower space by $$z = [M\\cdot y]$$ where M is a multivariant i,i,d Gaussian matrix, and $[]$ is tkaing the sign.\n",
    "\n",
    "Then we train binary classifiers on each bit of $z \\in \\{0, 1\\}^{\\hat L}$\n",
    "\n",
    "For each test point, we predict its $\\hat z$ and then recover the $\\hat y$ using BIHT algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&y^0=0^L\\\\\n",
    "&\\text{for t = 0, 1, 2 ...}\\\\\n",
    "&\\quad a^{t+1}=y^t+\\frac{\\tau}{2}M^T(z-[My^t])\\\\\n",
    "&\\quad y^{t+1}=\\eta_k(a^{t+1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\eta_k()$ here is keeping the k greatest positive value inside a and leaves others to 0. This algorithm can converge to the recovered $\\hat y$. \n",
    "\n",
    "We want to try k=99% sparsity, and $\\tau$ is a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#santy checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from BMapModel import BM_Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: data preposseing\n",
    "normalize features and select prominent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize features\n",
    "X_tr = StandardScaler().fit_transform(X_tr)\n",
    "X_te = StandardScaler().fit_transform(X_te)\n",
    "X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: map y to $\\hat L$ space z\n",
    "\n",
    "We choose $$\\hat L = k \\log L$$ where $k$ indicates the sparsity of each label vector $y_i = \\{0,1\\}^L$. By default we choose k to be the 99.9% maximum sparsity to avoid extreme cases.\n",
    "\n",
    "The data in \"Eurlex\" contains $L = 5000$ labels, we are trying to map it into $\\hat L = 200$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = sorted([Y.sum() for Y in Y_tr], reverse=True)[int(num_point*0.01)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_hat = int(math.ceil(k * math.log(Y_tr.shape[1], 2) * L_hat_ratio))\n",
    "L_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_tr = map_2_z(Y_tr, L_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model\n",
    "\n",
    "#### 2.1 train binary classifiers on each bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bit(bit):\n",
    "    print \"Trianning model for the {}th bit\\n... ... ... \\n\".format(bit)\n",
    "    #clf = LogisticRegression(solver='sag')\n",
    "    clf = LinearSVC(dual=False)\n",
    "    clf.fit(y=Z_tr[:, bit], X=X_tr)\n",
    "    joblib.dump(clf, os.path.join(model_path , 'label{}.pkl'.format(bit)))\n",
    "    print \"{}th bit's model successfully stored in {}/label{}.pkl\\n\".format(bit, model_path, bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parallel(n_jobs=num_core)(delayed(train_bit)(i) for i in range(Z_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Prediction and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BM_Predictor(Y_tr.shape[1], L_hat)\n",
    "model.load_clf(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time is 11.770727 seconds.\n"
     ]
    }
   ],
   "source": [
    "# k=1 without voting\n",
    "time.tic()\n",
    "Y_pred = model.predict_y(X_te, sparsity=k, recover='BIHT') # 1 nearest neighbor\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 983)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 983)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17174254317111459"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(Y_te, Y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#average_precision_score(y_true=Y_te, y_score=Y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_at_k_score = []\n",
    "for voter in range(1,100):\n",
    "    Y_pred = model.predict_y(X_te, voter, weighted=True)\n",
    "    p_at_k_score.append(precision_at_k(Y_te, Y_pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,100), p_at_k_score)\n",
    "plt.xlabel('number of voters in kNN')\n",
    "plt.ylabel('p@1 score')\n",
    "np.argmax(p_at_k_score)\n",
    "p_at_k_score[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 See the model performance under no error channel\n",
    "Given our predicted value is the correct \"Z_te\", what performance can our model achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_free(L_hat, Y_tr, Y_te, tau, iterate, pk):\n",
    "    z_te = map_2_z(Y_te, L_hat)\n",
    "    # faiss brute force search\n",
    "\n",
    "    model = BM_Predictor(Y_tr.shape[1], L_hat)\n",
    "\n",
    "    y_pred_fake = model.BIHT_y(z_te, sparsity=25, tau=tau, iterate=iterate)\n",
    "    return precision_at_k(Y_te, y_pred_fake, pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tau=.1;iterate=50;pk=1\n",
    "L_hat_free_score = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_free)(L_hat, Y_tr, Y_te, tau, iterate, pk) for L_hat in range(1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,300), L_hat_free_score)\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('p@1 score')\n",
    "plt.title('Delicious: The model capacity for BIHT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=0.01;L_hat=150;pk=1\n",
    "Iterate_free_score = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_free)(L_hat, Y_tr, Y_te, tau, iterate, pk) for iterate in range(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,20), Iterate_free_score)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('p@1 score')\n",
    "plt.title('Delicious: The model capacity for BIHT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_free(L_hat,Y_tr,Y_te,tau=0.005,iterate=200,pk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3 optimize hyperparameter\n",
    "use  k fold cross validation to optimize over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate the result with different L_hat under the same model\n",
    "def validate(L_hat, pk=1, vote=10): # simple forkable parallel for loop body\n",
    "    #k_fold = KFold(n_splits=fold)\n",
    "    #print \"L_hat is now {}\\n\".format(L_hat)\n",
    "    p_sum = 0\n",
    "   # for train_index, test_index in k_fold.split(X_tr):\n",
    "    x_train = X_tr\n",
    "    y_train = Y_tr\n",
    "    x_test = X_te\n",
    "    y_test = Y_te\n",
    "\n",
    "    # map and create kNN index\n",
    "    z_train = map_2_z(y_train, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(z_train.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(z_train.astype('float32'))\n",
    "\n",
    "    # construct model\n",
    "    model = BMapModel.BM_Predictor(L_hat, knn_index, y_train)\n",
    "    model.load_model(model_path)\n",
    "    #predict and calculate p@k score\n",
    "    y_pred = model.predict_y(x_test, vote, weighted=True)\n",
    "    # precision@pk\n",
    "    #p_sum += precision_at_k(y_test, y_pred, k=pk)\n",
    "    return precision_at_k(y_test, y_pred, k=pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize L_hat's value on the metric precision@k\n",
    "pk=1\n",
    "vote=40\n",
    "L_hat_range = range(1, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_hat_score = Parallel(n_jobs=num_core)(delayed(validate)(L_hat, pk, vote) for L_hat in L_hat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_up, = plt.plot(range(1,120), L_hat_free_score, label='Model Capacity')\n",
    "line_down, = plt.plot(range(1,120), L_hat_score, label='OvsA performance')\n",
    "plt.legend(handles=[line_up, line_down])\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('precision@{}'.format(pk))\n",
    "plt.title('Eurlex validation on L_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
