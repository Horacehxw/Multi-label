{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMap + BIHT(Binary Interactive Hard Thresholding)\n",
    "We are now recovering the BMap(1bit compressive sensing) using BIHT instead of lazy kNN.\n",
    "\n",
    "https://arxiv.org/pdf/1104.3160.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import data_util\n",
    "import BMapModel\n",
    "#from data_util import DataPoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import faiss\n",
    "# import joblib # version incompatibel with sklearn's joblib and can't load the previous model\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib # store classifiers\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # convert y to {0,1}^L\n",
    "from sklearn.preprocessing import StandardScaler # normalize features \n",
    "from sklearn.feature_extraction import DictVectorizer # extract feature vector to x\n",
    "from numpy.random import normal # generate transforming matrix\n",
    "from sklearn.neighbors import KDTree #KDTree for fast kNN search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from joblib import Parallel, delayed # Multitread\n",
    "from pytictoc import TicToc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data:\r\n",
      "AmazonCat  Delicious  Mediamill  README_Datasets  XMLDatasetRead\r\n",
      "Bibtex\t   Eurlex     RCV1-x\t Wiki10\r\n",
      "\r\n",
      "../data/AmazonCat:\r\n",
      "amazonCat_test.txt  amazonCat_train.txt\r\n",
      "\r\n",
      "../data/Bibtex:\r\n",
      "Bibtex_data.txt  bibtex_trSplit.txt  bibtex_tstSplit.txt\r\n",
      "\r\n",
      "../data/Delicious:\r\n",
      "Delicious_data.txt  delicious_trSplit.txt  delicious_tstSplit.txt\r\n",
      "\r\n",
      "../data/Eurlex:\r\n",
      "eurlex_test.txt  eurlex_train.txt\r\n",
      "\r\n",
      "../data/Mediamill:\r\n",
      "Mediamill_data.txt  mediamill_trSplit.txt  mediamill_tstSplit.txt\r\n",
      "\r\n",
      "../data/RCV1-x:\r\n",
      "rcv1x_test.txt\trcv1x_train.txt\r\n",
      "\r\n",
      "../data/Wiki10:\r\n",
      "wiki10_test.txt  wiki10_train.txt\r\n",
      "\r\n",
      "../data/XMLDatasetRead:\r\n",
      "XMLDatasetRead\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead:\r\n",
      "ReadData_Matlab  README_Datasets\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead/ReadData_Matlab:\r\n",
      "make.m\tread_data.cpp  README.txt  write_data.cpp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m../data/Delicious/Delicious_data.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/Delicious/Delicious_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "model_dir = \"../.model2\"\n",
    "train_filename = \"/Delicious/Delicious_data.txt\"\n",
    "#test_filename = \"/Eurlex/eurlex_test.txt\"\n",
    "tr_split_file = \"/Delicious/delicious_trSplit.txt\"\n",
    "te_split_file = \"/Delicious/delicious_tstSplit.txt\"\n",
    "\n",
    "path = os.path.dirname(train_filename)\n",
    "model_path = model_dir + path\n",
    "num_core = 8\n",
    "L_hat_ratio = 0.5\n",
    "time = TicToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_point=16105, num_label=983, num_feature=500\n"
     ]
    }
   ],
   "source": [
    "tr_data, num_point, num_feature, num_label = data_util.read_file(data_dir+train_filename)\n",
    "print(\"num_point={}, num_label={}, num_feature={}\".format(num_point, num_label, num_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#te_data, _, _, _ = data_util.read_file(data_dir+test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_split = data_util.split_data(data=tr_data, split_file=data_dir+tr_split_file)\n",
    "te_split = data_util.split_data(data=tr_data, split_file=data_dir+te_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, Y_tr, X_te, Y_te = data_util.data_transform(tr_split, te_split, num_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary mapping + BIHT\n",
    "data set $(x,y)^d$, where $y_i=\\{1, 0\\}^L$\n",
    "\n",
    "we want to map y into lower space by $$z = [M\\cdot y]$$ where M is a multivariant i,i,d Gaussian matrix, and $[]$ is tkaing the sign.\n",
    "\n",
    "Then we train binary classifiers on each bit of $z \\in \\{0, 1\\}^{\\hat L}$\n",
    "\n",
    "For each test point, we predict its $\\hat z$ and then recover the $\\hat y$ using BIHT algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&y^0=0^L\\\\\n",
    "&\\text{for t = 0, 1, 2 ...}\\\\\n",
    "&\\quad a^{t+1}=y^t+\\frac{\\tau}{2}M^T(z-[My^t])\\\\\n",
    "&\\quad y^{t+1}=\\eta_k(a^{t+1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\eta_k()$ here is keeping the k greatest positive value inside a and leaves others to 0. This algorithm can converge to the recovered $\\hat y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: data preposseing\n",
    "normalize features and select prominent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize features\n",
    "X_tr = StandardScaler().fit_transform(X_tr)\n",
    "X_te = StandardScaler().fit_transform(X_te)\n",
    "X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: map to $\\hat L$ space and kNN search index\n",
    "\n",
    "The apply $$\\hat L = k \\log L$$ where $k$ indicates the sparsity of each label vector $y_i = \\{0,1\\}^L$. By default we choose k to be the 99.9% maximum sparsity to avoid extreme cases.\n",
    "\n",
    "The data in \"Eurlex\" contains $L = 5000$ labels, we are trying to map it into $\\hat L = 200$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = sorted([Y.sum() for Y in Y_tr], reverse=True)[int(num_point*0.0001)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_hat = int(math.ceil(k * math.log(Y_tr.shape[1], 2) * L_hat_ratio))\n",
    "L_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_2_z(Y, L_hat):\n",
    "    np.random.seed(0)\n",
    "    M = normal(size=(L_hat, Y.shape[1]))\n",
    "    Z = M.dot(Y.T).T # z = n*\\hat L\n",
    "    Z = np.apply_along_axis(lambda x: [0 if elem < 0 else 1 for elem in x], 0, Z) #sign\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_tr = map_2_z(Y_tr, L_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model\n",
    "\n",
    "#### 2.1 train binary classifiers on each bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bit(bit):\n",
    "    print \"Trianning model for the {}th bit\\n... ... ... \\n\".format(bit)\n",
    "    #clf = LogisticRegression(solver='sag')\n",
    "    clf = LinearSVC(dual=False)\n",
    "    clf.fit(y=Z_tr[:, bit], X=X_tr)\n",
    "    joblib.dump(clf, os.path.join(model_path , 'label{}.pkl'.format(bit)))\n",
    "    print \"{}th bit's model successfully stored in {}/label{}.pkl\\n\".format(bit, model_path, bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed # Multitread\n",
    "Parallel(n_jobs=num_core)(delayed(train_bit)(i) for i in range(Z_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.2 Store the lower degree space info for kNN\n",
    "\n",
    "We use opensource faiss library from FAIR to speedup the ANN(Approximate Nearest Neighbor) search.\n",
    "\n",
    "When dimension and data size is relatively small, we use the brute force kNN search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-59819b700ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# faiss brute force search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Flat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC_L2\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# build the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnn_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "# faiss brute force search\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "nn_index.add(Z_tr.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Python\n",
    "# index created by index factory\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"IVF100,Flat\", faiss.METRIC_L2) # need train\n",
    "nn_index.train(Z_tr.astype('float32'))\n",
    "nn_index.add(Z_tr.astype('float32'))\n",
    "\n",
    "print nn_index.nlist # number of clusters, only INF has this\n",
    "nn_index.nprobe = 1 # number of clusters to search through, only INF has this, need to be validate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Prediction and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BMapModel.BM_Predictor(L_hat, nn_index, Y_tr)\n",
    "model.load_clf(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=1 without voting\n",
    "time.tic()\n",
    "Y_pred = model.predict_y(X_te, 20) # 1 nearest neighbor\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#average_precision_score(y_true=Y_te, y_score=Y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_at_k(truth, vote, k):\n",
    "    '''\n",
    "    evaluate precision at k for a vote vector\n",
    "    p@k = num of correct prediction in topk / k\n",
    "    '''\n",
    "    success = 0\n",
    "    for i in range(len(truth)):\n",
    "        # find the k-largest index using partition selet\n",
    "        # topk are not sorted, np.argsort(vote[topk]) can do that but not needed here\n",
    "        topk = np.argpartition(vote[i], -k)[-k:] \n",
    "        success += np.sum(truth[i, topk])\n",
    "    return success / ((float(len(truth)))*k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_at_k_score = []\n",
    "for voter in range(1,100):\n",
    "    Y_pred = model.predict_y(X_te, voter, weighted=True)\n",
    "    p_at_k_score.append(precision_at_k(Y_te, Y_pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,100), p_at_k_score)\n",
    "plt.xlabel('number of voters in kNN')\n",
    "plt.ylabel('p@1 score')\n",
    "np.argmax(p_at_k_score)\n",
    "p_at_k_score[40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 See the model performance under no error channel\n",
    "Given our predicted value is the correct \"Z_te\", what performance can our model achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_free(L_hat, Y_tr, Y_te, pk=1, vote=10):\n",
    "    Z_tr = map_2_z(Y_tr, L_hat)\n",
    "    z_te = map_2_z(Y_te, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(Z_tr.astype('float32'))\n",
    "\n",
    "    model = BMapModel.BM_Predictor(L_hat, knn_index, Y_tr)\n",
    "\n",
    "    y_pred_fake = model.vote_y(z_te, vote, weighted=False)\n",
    "    return precision_at_k(Y_te, y_pred_fake, pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pk=1;vote=10\n",
    "L_hat_free_score = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_free)(L_hat, Y_tr, Y_te, pk, vote) for L_hat in range(1, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,120), L_hat_free_score)\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('p@3 score')\n",
    "plt.title('Eurlex: The model capacity for Bmap and kNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3 optimize hyperparameter\n",
    "use  k fold cross validation to optimize over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate the result with different L_hat under the same model\n",
    "def validate(L_hat, pk=1, vote=10): # simple forkable parallel for loop body\n",
    "    #k_fold = KFold(n_splits=fold)\n",
    "    #print \"L_hat is now {}\\n\".format(L_hat)\n",
    "    p_sum = 0\n",
    "   # for train_index, test_index in k_fold.split(X_tr):\n",
    "    x_train = X_tr\n",
    "    y_train = Y_tr\n",
    "    x_test = X_te\n",
    "    y_test = Y_te\n",
    "\n",
    "    # map and create kNN index\n",
    "    z_train = map_2_z(y_train, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(z_train.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(z_train.astype('float32'))\n",
    "\n",
    "    # construct model\n",
    "    model = BMapModel.BM_Predictor(L_hat, knn_index, y_train)\n",
    "    model.load_model(model_path)\n",
    "    #predict and calculate p@k score\n",
    "    y_pred = model.predict_y(x_test, vote, weighted=True)\n",
    "    # precision@pk\n",
    "    #p_sum += precision_at_k(y_test, y_pred, k=pk)\n",
    "    return precision_at_k(y_test, y_pred, k=pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize L_hat's value on the metric precision@k\n",
    "pk=1\n",
    "vote=40\n",
    "L_hat_range = range(1, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_hat_score = Parallel(n_jobs=num_core)(delayed(validate)(L_hat, pk, vote) for L_hat in L_hat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_up, = plt.plot(range(1,120), L_hat_free_score, label='Model Capacity')\n",
    "line_down, = plt.plot(range(1,120), L_hat_score, label='OvsA performance')\n",
    "plt.legend(handles=[line_up, line_down])\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('precision@{}'.format(pk))\n",
    "plt.title('Eurlex validation on L_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
