{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise label MMSE\n",
    "https://www.overleaf.com/11871051tmqzsmsbdxrk#/44989926/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import data_util\n",
    "import BMapModel\n",
    "#from data_util import DataPoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import faiss\n",
    "import util\n",
    "import scipy\n",
    "# import joblib # version incompatibel with sklearn's joblib and can't load the previous model\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.externals import joblib # store classifiers\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # convert y to {0,1}^L\n",
    "from sklearn.preprocessing import StandardScaler # normalize features \n",
    "from sklearn.feature_extraction import DictVectorizer # extract feature vector to x\n",
    "from numpy.random import normal # generate transforming matrix\n",
    "from sklearn.neighbors import KDTree #KDTree for fast kNN search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from joblib import Parallel, delayed # Multitread\n",
    "from pytictoc import TicToc\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data:\r\n",
      "AmazonCat\t   Bibtex\t   Eurlex     README_Datasets\r\n",
      "AmazonCat-14K\t   Delicious\t   Mediamill  Wiki10\r\n",
      "AmazonCat-14K.zip  DeliciousLarge  RCV1-x     XMLDatasetRead\r\n",
      "\r\n",
      "../data/AmazonCat:\r\n",
      "amazonCat_test.txt  amazonCat_train.txt  X_te.npz  X_tr.npz  Y_te.npz  Y_tr.npz\r\n",
      "\r\n",
      "../data/AmazonCat-14K:\r\n",
      "amazonCat-14K_test.txt\t X_te.npz  Y_te.npz\r\n",
      "amazonCat-14K_train.txt  X_tr.npz  Y_tr.npz\r\n",
      "\r\n",
      "../data/Bibtex:\r\n",
      "Bibtex_data.txt     bibtex_tstSplit.txt  X_tr.npz  Y_tr.npz\r\n",
      "bibtex_trSplit.txt  X_te.npz\t\t Y_te.npz\r\n",
      "\r\n",
      "../data/Delicious:\r\n",
      "Delicious_data.txt\tX_te.npz  X_tr.pkl  Y_tr.npz\r\n",
      "delicious_trSplit.txt\tX_te.pkl  Y_te.npz  Y_tr.pkl\r\n",
      "delicious_tstSplit.txt\tX_tr.npz  Y_te.pkl\r\n",
      "\r\n",
      "../data/DeliciousLarge:\r\n",
      "deliciousLarge_test.txt   X_te.npz  Y_te.npz\r\n",
      "deliciousLarge_train.txt  X_tr.npz  Y_tr.npz\r\n",
      "\r\n",
      "../data/Eurlex:\r\n",
      "eurlex_test.txt  eurlex_train.txt  X_te.npz  X_tr.npz  Y_te.npz  Y_tr.npz\r\n",
      "\r\n",
      "../data/Mediamill:\r\n",
      "Mediamill_data.txt  mediamill_trSplit.txt  mediamill_tstSplit.txt\r\n",
      "\r\n",
      "../data/RCV1-x:\r\n",
      "rcv1x_test.txt\t X_te.npz  X_tr.npz  Y_te.npz  Y_tr.npz\r\n",
      "rcv1x_train.txt  X_te.pkl  X_tr.pkl  Y_te.pkl  Y_tr.pkl\r\n",
      "\r\n",
      "../data/Wiki10:\r\n",
      "wiki10_test.txt   X_te.npz  X_tr.npz  Y_te.npz\tY_tr.npz\r\n",
      "wiki10_train.txt  X_te.pkl  X_tr.pkl  Y_te.pkl\tY_tr.pkl\r\n",
      "\r\n",
      "../data/XMLDatasetRead:\r\n",
      "XMLDatasetRead\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead:\r\n",
      "ReadData_Matlab  README_Datasets\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead/ReadData_Matlab:\r\n",
      "make.m\tread_data.cpp  README.txt  write_data.cpp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m../data/Delicious/Delicious_data.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/Delicious/Delicious_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "model_dir = \"../noise_model/model1\"\n",
    "path = '/Delicious'\n",
    "\n",
    "data_path = data_dir+path\n",
    "clean_model_path = model_dir + path +'/clean'\n",
    "noise_model_path = model_dir + path +'/noise'\n",
    "num_core = -1\n",
    "L_hat = 100\n",
    "time = TicToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[X_tr, X_te, Y_tr, Y_te] = [load_npz(os.path.join(data_path, '{}.npz'.format(name)))\\\n",
    "                            for name in ['X_tr', 'X_te', 'Y_tr', 'Y_te']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12920, 500), (3185, 500), (12920, 983), (3185, 983))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape, Y_tr.shape, Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use np array for simplicity\n",
    "[X_tr, X_te, Y_tr, Y_te] = [x.toarray() for x in [X_tr, X_te, Y_tr, Y_te]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Noise to Label\n",
    "We can assume that $\\tilde{y}_i$ is generated by flipping the elements in $y^i$ with some probability. In particular, we assume that when $y^i_{j}=0$, $P({\\tilde{y}^i_{j}=1\\mid y^i_{j}=0}) = \\rho_+$, and when $y^i_{j}=1$, $P({\\tilde{y}^i_{j}=0\\mid y^i_{j}=1}) = \\rho_-$. We assume the label flip probabilities are the same across all the data points and classes.\n",
    "\n",
    "Lets assume $\\rho+=0.01, \\rho-=0.5$ for example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_bits(message, p0, p1):\n",
    "    '''\n",
    "    randomly flip every \"0->1\" w/ prob p1, and every \"1->0\" w/ p0\n",
    "    '''\n",
    "    def flip(bit):\n",
    "        if bit==1 and np.random.rand()<p0:\n",
    "            bit = 0\n",
    "        if bit==0 and np.random.rand()<p1:\n",
    "            bit=1\n",
    "        return bit\n",
    "    np.random.seed(0)\n",
    "    return np.apply_along_axis(lambda bits: np.array([flip(bit) for bit in bits]), 0, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RHO0, RHO1=0.3, 0.1\n",
    "Y_tr_flip = flip_bits(Y_tr, RHO0, RHO1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover Label $\\hat y$ from the noise label using MMSE\n",
    "\n",
    "$$\\hat{y}_j = E(y_j\\mid \\tilde{y}_j) = \\frac{P(y_j=1)P(\\tilde{y}_j\\mid y_j=1)}{P(\\tilde{y}_j)} = \\begin{cases}\n",
    "\\frac{P(y_j=1)}{P(\\tilde{y}_j=1)} (1-\\rho_-) & \\text{ if } \\tilde{y}_j = 1 \\\\\n",
    "\\frac{P(y_j=1)}{P(\\tilde{y}_j=0)} \\rho_- & \\text{ if } \\tilde{y}_j = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Let $$\\hat y_1=\\frac{P(y_j=1)}{P(\\tilde{y}_j=1)} (1-\\rho_-)$$\n",
    "$$\\hat y_2=\\frac{P(y_j=1)}{P(\\tilde{y}_j=0)} \\rho_-$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MMSE(Y_flip, p0, p1):\n",
    "    prob1_hat = np.sum(Y_flip)/float(Y_flip.shape[0]*Y_flip.shape[1])\n",
    "    prob1 = (prob1_hat-p1)/(1-p1-p0)\n",
    "    y_hat_1=prob1/prob1_hat*(1-p0)\n",
    "    y_hat_0 = prob1/(1-prob1_hat)*p0\n",
    "    return np.apply_along_axis(lambda bits: np.array([y_hat_1 if bit==1 else y_hat_0 for bit in bits]), \n",
    "                               0, Y_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_tr_mmse = MMSE(Y_tr_flip, RHO0, RHO1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Z_tr, Z_tr_flip, Z_tr_mmse] = [util.map_2_z(Y, L_hat) for Y in [Y_tr, Y_tr_flip, Y_tr_mmse]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59525464396284833"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Z_tr==Z_tr_flip).sum()/float(Z_tr.shape[0]*Z_tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59572600619195049"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Z_tr==Z_tr_mmse).sum()/float(Z_tr.shape[0]*Z_tr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95091408668730648"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Z_tr_flip==Z_tr_mmse).sum()/float(Z_tr.shape[0]*Z_tr.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model\n",
    "\n",
    "#### 2.1 train binary classifiers on each bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_bit(bit):\n",
    "    print \"Trianning model for the {}th bit\\n... ... ... \\n\".format(bit)\n",
    "    #clf = LogisticRegression(solver='sag')\n",
    "    clf = LinearSVC(dual=False)\n",
    "    clf.fit(y=Z_tr[:, bit], X=X_tr)\n",
    "    joblib.dump(clf, os.path.join(model_path , 'label{}.pkl'.format(bit)))\n",
    "    print \"{}th bit's model successfully stored in {}/label{}.pkl\\n\".format(bit, model_path, bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed # Multitread\n",
    "#Parallel(n_jobs=num_core)(delayed(train_bit)(i) for i in range(Z_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.2 Store the lower degree space info for kNN\n",
    "\n",
    "We use opensource faiss library from FAIR to speedup the ANN(Approximate Nearest Neighbor) search.\n",
    "\n",
    "When dimension and data size is relatively small, we use the brute force kNN search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# faiss brute force search\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "nn_index.add(Z_tr.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Python\n",
    "# index created by index factory\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"IVF100,Flat\", faiss.METRIC_L2) # need train\n",
    "nn_index.train(Z_tr.astype('float32'))\n",
    "nn_index.add(Z_tr.astype('float32'))\n",
    "\n",
    "print nn_index.nlist # number of clusters, only INF has this\n",
    "nn_index.nprobe = 1 # number of clusters to search through, only INF has this, need to be validate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Prediction and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat, index=nn_index, Y_tr=Y_tr, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k=1 without voting\n",
    "time.tic()\n",
    "Y_pred = model.predict_y(X_te, 20, vote=40) # 1 nearest neighbor\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#average_precision_score(y_true=Y_te, y_score=Y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "util.precision_at_k(Y_te, Y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_voter(voter):\n",
    "    Y_pred = model.predict_y(X_te, vote=voter, weighted=True)\n",
    "    return (util.precision_at_k(Y_te, Y_pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_at_k_votes = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_voter)(voter) for voter in range(1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,100), p_at_k_votes)\n",
    "plt.xlabel('number of voters in kNN')\n",
    "plt.ylabel('p@1 score')\n",
    "plt.title('OvsA, L_hat={}, {}'.format(L_hat, train_filename))\n",
    "top = np.argmax(p_at_k_votes)\n",
    "p_at_k_votes[top], top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 See the model performance under no error channel\n",
    "Given our predicted value is the correct \"Z_te\", what performance can our model achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(L_hat, Y_tr, Y_te, pk=1, vote=10):\n",
    "    Z_tr = util.map_2_z(Y_tr, L_hat)\n",
    "    z_te = util.map_2_z(Y_te, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(Z_tr.astype('float32'))\n",
    "\n",
    "    model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat, index=knn_index, Y_tr=Y_tr)\n",
    "\n",
    "    y_pred_fake = model.vote_y(z_te, vote=vote, weighted=False)\n",
    "    return util.precision_at_k(Y_te, y_pred_fake, pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pk=1;vote=40\n",
    "L_hat_free_score = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_model)(L_hat, Y_tr, Y_te, pk, vote) for L_hat in range(1, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,120), L_hat_free_score)\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('p@1 score')\n",
    "plt.title('Eurlex: The model capacity for Bmap and kNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3 optimize hyperparameter\n",
    "use  k fold cross validation to optimize over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validate the result with different L_hat under the same model\n",
    "def validate(L_hat, pk=1, vote=20): # simple forkable parallel for loop body\n",
    "    from util import map_2_z\n",
    "    from util import precision_at_k\n",
    "    #k_fold = KFold(n_splits=fold)\n",
    "    #print \"L_hat is now {}\\n\".format(L_hat)\n",
    "    p_sum = 0\n",
    "   # for train_index, test_index in k_fold.split(X_tr):\n",
    "    x_train = X_tr\n",
    "    y_train = Y_tr\n",
    "    x_test = X_te\n",
    "    y_test = Y_te\n",
    "\n",
    "    # map and create kNN index\n",
    "    z_train = map_2_z(y_train, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(z_train.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(z_train.astype('float32'))\n",
    "\n",
    "    # construct model\n",
    "    model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat, index=knn_index, Y_tr=y_train)\n",
    "    model.load_clf(model_path)\n",
    "    #predict and calculate p@k score\n",
    "    y_pred = model.predict_y(x_test, vote=vote, weighted=True)\n",
    "    # precision@pk\n",
    "    #p_sum += precision_at_k(y_test, y_pred, k=pk)\n",
    "    return precision_at_k(y_test, y_pred, k=pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize L_hat's value on the metric precision@k\n",
    "pk=1\n",
    "vote=40\n",
    "L_hat_range = range(1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_hat_score = Parallel(n_jobs=num_core)(delayed(validate)(L_hat, pk, vote) for L_hat in L_hat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_up, = plt.plot(range(1,200), L_hat_free_score, label='Model Capacity')\n",
    "line_down, = plt.plot(range(1,200), L_hat_score, label='OvsA performance')\n",
    "plt.legend(handles=[line_up, line_down])\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('precision@{}'.format(pk))\n",
    "plt.title('Delicious validation on L_hat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Bit Flip Probability\n",
    "the classifiers predict $\\hat z$ can be viewed as transmiting z from a BSC channel with some bit flip probability, this is actually representing the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_hat, X_te.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_channel(X_te, Y_te):\n",
    "    z_te = util.map_2_z(Y_te, L_hat)\n",
    "    # use the classifers to predict z_hat\n",
    "    model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat, model_path=model_path)\n",
    "    z_pred = model.predict_z(X_te)\n",
    "    \n",
    "    hamming = []\n",
    "    for i in range(z_te.shape[0]):\n",
    "        hamming.append((z_pred[i]!=z_te[i]).sum())\n",
    "    return np.array(hamming) / float(z_te.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_error = validate_channel(X_te, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(test_error, bins=20)\n",
    "plt.xlabel('error rate between z_te and z_pred')\n",
    "plt.ylabel('density')\n",
    "plt.title('Delicious: distribution of bit_flip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_error = validate_channel(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(training_error, bins=20)\n",
    "plt.xlabel('error rate between z_train and z_pred')\n",
    "plt.ylabel('density')\n",
    "plt.title('Delicious: distribution of training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_error.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5 Train and test model directly on the X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir_mirror = model_dir+\"_origin\"\n",
    "model_path_mirror = model_dir_mirror+path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
