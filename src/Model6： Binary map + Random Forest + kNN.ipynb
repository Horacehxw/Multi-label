{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M6ï¼š Binary map + Random Forest + kNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import data_util\n",
    "import BMapModel\n",
    "#from data_util import DataPoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import faiss\n",
    "import util\n",
    "# import joblib # version incompatibel with sklearn's joblib and can't load the previous model\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib # store classifiers\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # convert y to {0,1}^L\n",
    "from sklearn.preprocessing import StandardScaler # normalize features \n",
    "from sklearn.feature_extraction import DictVectorizer # extract feature vector to x\n",
    "from numpy.random import normal # generate transforming matrix\n",
    "from sklearn.neighbors import KDTree #KDTree for fast kNN search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from joblib import Parallel, delayed # Multitread\n",
    "from pytictoc import TicToc\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data:\r\n",
      "AmazonCat  Delicious  Mediamill  README_Datasets  XMLDatasetRead\r\n",
      "Bibtex\t   Eurlex     RCV1-x\t Wiki10\r\n",
      "\r\n",
      "../data/AmazonCat:\r\n",
      "amazonCat_test.txt  amazonCat_train.txt\r\n",
      "\r\n",
      "../data/Bibtex:\r\n",
      "Bibtex_data.txt  bibtex_trSplit.txt  bibtex_tstSplit.txt\r\n",
      "\r\n",
      "../data/Delicious:\r\n",
      "Delicious_data.txt  delicious_trSplit.txt  delicious_tstSplit.txt\r\n",
      "\r\n",
      "../data/Eurlex:\r\n",
      "eurlex_test.txt  eurlex_train.txt\r\n",
      "\r\n",
      "../data/Mediamill:\r\n",
      "Mediamill_data.txt  mediamill_trSplit.txt  mediamill_tstSplit.txt\r\n",
      "\r\n",
      "../data/RCV1-x:\r\n",
      "rcv1x_test.txt\trcv1x_train.txt\r\n",
      "\r\n",
      "../data/Wiki10:\r\n",
      "wiki10_test.txt  wiki10_train.txt\r\n",
      "\r\n",
      "../data/XMLDatasetRead:\r\n",
      "XMLDatasetRead\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead:\r\n",
      "ReadData_Matlab  README_Datasets\r\n",
      "\r\n",
      "../data/XMLDatasetRead/XMLDatasetRead/ReadData_Matlab:\r\n",
      "make.m\tread_data.cpp  README.txt  write_data.cpp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m../data/RCV1-x/rcv1x_test.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/RCV1-x/rcv1x_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "model_dir = \"../.model6\"\n",
    "train_filename = \"/RCV1-x/rcv1x_train.txt\"\n",
    "test_filename = \"/RCV1-x/rcv1x_test.txt\"\n",
    "#tr_split_file = \"/Delicious/delicious_trSplit.txt\"\n",
    "#te_split_file = \"/Delicious/delicious_tstSplit.txt\"\n",
    "\n",
    "path = os.path.dirname(train_filename)\n",
    "model_path = model_dir + path\n",
    "num_core = -1\n",
    "L_hat_ratio = 0.5\n",
    "time = TicToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_point=623847, num_label=2456, num_feature=47236\n"
     ]
    }
   ],
   "source": [
    "tr_data, num_point, num_feature, num_label = data_util.read_file(data_dir+train_filename)\n",
    "print(\"num_point={}, num_label={}, num_feature={}\".format(num_point, num_label, num_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te_data, _, _, _ = data_util.read_file(data_dir+test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tr_split = data_util.split_data(data=tr_data, split_file=data_dir+tr_split_file)\n",
    "#te_split = data_util.split_data(data=tr_data, split_file=data_dir+te_split_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2078473913bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/datadrive/Multi-label/src/data_util.pyc\u001b[0m in \u001b[0;36mdata_transform\u001b[0;34m(tr, te, num_label)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mX_te_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mfv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mX_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mX_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/horacehxw/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tosequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mXa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_tr, Y_tr, X_te, Y_te = data_util.data_transform(tr_data, te_data, num_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary mapping + kNN\n",
    "data set $(x,y)^d$, where $y_i=\\{1, 0\\}^L$\n",
    "\n",
    "we want to map y into lower space by $$z = [M\\cdot y]$$ where M is a multivariant i,i,d Gaussian matrix, and $[]$ is tkaing the sign.\n",
    "\n",
    "Then we train binary classifiers on each bit of $z \\in \\{0, 1\\}^{\\hat L}$\n",
    "\n",
    "For each test point, we predict its $\\hat z$ and then use kNN to find the nearest k neighbors from $z=[My]$ which is all our lower degree space's mapping.\n",
    "\n",
    "### Step 0: data preposseing\n",
    "normalize features and select prominent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "X_tr = StandardScaler().fit_transform(X_tr)\n",
    "X_te = StandardScaler().fit_transform(X_te)\n",
    "X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: map to $\\hat L$ space and kNN search index\n",
    "\n",
    "We apply $$\\hat L = k \\log L$$ where $k$ indicates the sparsity of each label vector $y_i = \\{0,1\\}^L$. By default we choose k to be the 99.9% maximum sparsity to avoid extreme cases.\n",
    "\n",
    "The data in \"Eurlex\" contains $L = 5000$ labels, we are trying to map it into $\\hat L = 200$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sorted([Y.sum() for Y in Y_tr], reverse=True)[int(num_point*0.0001)]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_hat = int(math.ceil(k * math.log(Y_tr.shape[1], 2) * L_hat_ratio))\n",
    "L_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_tr = util.map_2_z(Y_tr, L_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Model\n",
    "\n",
    "#### 2.1 train binary classifiers on each bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=1)\n",
    "time.tic()\n",
    "clf.fit(X_tr, Z_tr)\n",
    "time.toc()\n",
    "joblib.dump(clf, os.path.join(model_path , 'label0.pkl'))# only one classifiers, name for convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.2 Store the lower degree space info for kNN\n",
    "\n",
    "We use opensource faiss library from FAIR to speedup the ANN(Approximate Nearest Neighbor) search.\n",
    "\n",
    "When dimension and data size is relatively small, we use the brute force kNN search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# faiss brute force search\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "nn_index.add(Z_tr.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Python\n",
    "# index created by index factory\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"IVF100,Flat\", faiss.METRIC_L2) # need train\n",
    "nn_index.train(Z_tr.astype('float32'))\n",
    "nn_index.add(Z_tr.astype('float32'))\n",
    "\n",
    "print nn_index.nlist # number of clusters, only INF has this\n",
    "nn_index.nprobe = 1 # number of clusters to search through, only INF has this, need to be validate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Prediction and Validation\n",
    "\n",
    "test by RandomForest.predict and predict_prob to generate z_pred and then ues kNN to find y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat=1, index=nn_index, Y_tr=Y_tr, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time.tic()\n",
    "Y_pred = model.predict_y(X_te, vote=100, classifier='RandomForest') # 1 nearest neighbor\n",
    "time.toc()\n",
    "util.precision_at_k(Y_te, Y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_prob = model.predict_prob_z(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.tic()\n",
    "Y_pred = model.predict_y(X_te,  vote=100, classifier='RandomForest', predict_prob=True) # 1 nearest neighbor\n",
    "time.toc()\n",
    "util.precision_at_k(Y_te, Y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_voter(voter, use_prob):\n",
    "    Y_pred = model.predict_y(X_te, vote=voter, classifier='RandomForest', predict_prob=use_prob)\n",
    "    return (util.precision_at_k(Y_te, Y_pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_at_k_votes_prob = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_voter)(voter, True) for voter in range(1, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_at_k_votes = Parallel(n_jobs=num_core)\\\n",
    "                    (delayed(validate_voter)(voter, False) for voter in range(1, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,150), p_at_k_votes_prob, label='pred_prob')\n",
    "plt.plot(range(1,150), p_at_k_votes, label='pred')\n",
    "plt.xlabel('number of voters in kNN')\n",
    "plt.ylabel('p@1 score')\n",
    "plt.title('L_hat={}, RandomForest predict_prob, {}'.format(L_hat, train_filename))\n",
    "plt.legend()\n",
    "top = np.argmax(p_at_k_votes)\n",
    "(p_at_k_votes[top], top), (np.max(p_at_k_votes_prob), np.argmax(p_at_k_votes_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3 optimize hyperparameter\n",
    "use  k fold cross validation to optimize over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# validate the result with different L_hat under the same model\n",
    "def validate(L_hat, pk=1, vote=20, use_prob=False): # simple forkable parallel for loop body\n",
    "    from util import map_2_z\n",
    "    from util import precision_at_k\n",
    "\n",
    "   # for train_index, test_index in k_fold.split(X_tr):\n",
    "    x_train = X_tr\n",
    "    y_train = Y_tr\n",
    "    x_test = X_te\n",
    "    y_test = Y_te\n",
    "\n",
    "    # map and create kNN index\n",
    "    z_train = map_2_z(y_train, L_hat)\n",
    "    # faiss brute force search\n",
    "    knn_index = faiss.index_factory(z_train.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "    knn_index.add(z_train.astype('float32'))\n",
    "    #train clf\n",
    "    clf = RandomForestClassifier(random_state=1, n_estimators=100)\n",
    "    clf.fit(x_train, z_train)\n",
    "    # construct model\n",
    "    model = BMapModel.BM_Predictor(Y_tr.shape[1], 1, index=knn_index, Y_tr=y_train)\n",
    "    model.clfs.append(clf)\n",
    "    #predict and calculate p@k score\n",
    "    y_pred = model.predict_y(x_test, vote=vote, weighted=True, classifier='RandomForest', predict_prob=use_prob)\n",
    "    # precision@pk\n",
    "    return precision_at_k(y_test, y_pred, k=pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimize L_hat's value on the metric precision@k\n",
    "pk=1\n",
    "vote=100\n",
    "L_hat_range = range(1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_hat_score = Parallel(n_jobs=num_core)(delayed(validate)(L_hat, pk, vote) for L_hat in L_hat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_hat_score_prob = Parallel(n_jobs=num_core)(delayed(validate)(L_hat, pk, vote, True) for L_hat in L_hat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_down, = plt.plot(range(1,200), L_hat_score, label='random forest performance')\n",
    "plt.xlabel('L_hat')\n",
    "plt.ylabel('precision@{}'.format(pk))\n",
    "plt.title('Delicious validation on L_hat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Bit Flip Probability\n",
    "the classifiers predict $\\hat z$ can be viewed as transmiting z from a BSC channel with some bit flip probability, this is actually representing the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_hat, X_te.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_channel(X_te, Y_te):\n",
    "    z_te = util.map_2_z(Y_te, L_hat)\n",
    "    # use the classifers to predict z_hat\n",
    "    model = BMapModel.BM_Predictor(Y_tr.shape[1], L_hat=1, model_path=model_path)\n",
    "    z_pred = model.predict_z(X_te)\n",
    "    \n",
    "    hamming = []\n",
    "    for i in range(z_te.shape[0]):\n",
    "        hamming.append((z_pred[i]!=z_te[i]).sum())\n",
    "    return np.array(hamming) / float(z_te.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_error = validate_channel(X_te, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_error, bins=20)\n",
    "plt.xlabel('error rate between z_te and z_pred')\n",
    "plt.ylabel('density')\n",
    "plt.title('Delicious: distribution of bit_flip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_error = validate_channel(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(training_error, bins=20)\n",
    "plt.xlabel('error rate between z_train and z_pred')\n",
    "plt.ylabel('density')\n",
    "plt.title('{}: distribution of training error'.format(train_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5 Train and test model directly on the X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir_mirror = model_dir+\"_origin\"\n",
    "model_path_mirror = model_dir_mirror+path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=-1)\n",
    "time.tic()\n",
    "classifier.fit(X=X_tr, y=Y_tr)\n",
    "time.toc()\n",
    "#joblib.dump(classifier, os.path.join(model_path_mirror , 'RandomForestClassifier.pkl'))# only one classifiers, name for convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classifier = joblib.load(os.path.join(model_path_mirror , 'RandomForestClassifier.pkl'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.tic()\n",
    "y_pred = classifier.predict(X_te)\n",
    "time.toc('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamming = []\n",
    "for i in range(y_pred.shape[0]):\n",
    "    hamming.append((y_pred[i]!=Y_te[i]).sum())\n",
    "hamming = np.array(hamming) / float(Y_te.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(hamming)\n",
    "plt.xlabel('error_rate')\n",
    "plt.ylabel('density')\n",
    "plt.title('test error rate distribution, {}, Randomforest, mean={}'.format(train_filename, hamming.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.tic()\n",
    "y_pred_prob = classifier.predict_proba(X_te) #for every label there's a 2D (sample, output(2)) probability\n",
    "time.toc('predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_prob = np.ascontiguousarray(np.array([prob[:, 1] for prob in y_pred_prob]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the precision@k of RandomForest.predict_prob training directly on X_tr and Y_tr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.precision_at_k(Y_te, y_prob, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "precit_prob + kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_tr_index = faiss.index_factory(Y_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "Y_tr_index.add(Y_tr.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist, ind = Y_tr_index.search(y_prob.astype('float32'), 1)\n",
    "y_prob_vote = np.array([np.sum([Y_tr[ind[i][j]] for j in range(len(ind[i]))], axis=0) for i in range(len(ind))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.precision_at_k(Y_te, y_prob_vote, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_bits(message, p0, p1):\n",
    "    '''\n",
    "    randomly flip every \"1\" w/ prob p1, and every \"0\" w/ p0\n",
    "    '''\n",
    "    def flip(bit):\n",
    "        if bit==1 and np.random.rand()<p1:\n",
    "            bit = 0\n",
    "        if bit==0 and np.random.rand()<p0:\n",
    "            bit=1\n",
    "        return bit\n",
    "    np.random.seed(0)\n",
    "    return np.apply_along_axis(lambda bits: np.array([flip(bit) for bit in bits]), 0, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_bits(np.array([[1,0,1],[0,1,1]]), 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
