{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load GPU Faiss: No module named swigfaiss_gpu\n",
      "Faiss falling back to CPU-only.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import data_util\n",
    "import BMapModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import faiss\n",
    "import util\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from sklearn.externals import joblib # store classifiers\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from numpy.random import normal # generate transforming matrix\n",
    "from tensorflow.python.framework import function\n",
    "from joblib import Parallel, delayed # Multitread\n",
    "from pytictoc import TicToc\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.io as sio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mAmazonCat\u001b[0m/          \u001b[01;34mBibtex\u001b[0m/          \u001b[01;34mEurlex\u001b[0m/     \u001b[01;32mREADME_Datasets\u001b[0m*\r\n",
      "\u001b[34;42mAmazonCat-14K\u001b[0m/      \u001b[01;34mDelicious\u001b[0m/       \u001b[01;34mMediamill\u001b[0m/  \u001b[01;34mWiki10\u001b[0m/\r\n",
      "\u001b[01;32mAmazonCat-14K.zip\u001b[0m*  \u001b[01;34mDeliciousLarge\u001b[0m/  \u001b[01;34mRCV1-x\u001b[0m/     \u001b[01;34mXMLDatasetRead\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "model_dir = \"../model/model7\"\n",
    "path='/Eurlex'\n",
    "\n",
    "model_path = model_dir + path\n",
    "data_path = data_dir + path\n",
    "tf_board_path = '/tmp/tensorflow/SGH_multilabel' + path\n",
    "num_core = -1\n",
    "L_hat_ratio = 0.5 # useful when calculate L_hat = klogn*ratio\n",
    "L_hat = 100\n",
    "time = TicToc()\n",
    "[X_tr, X_te, Y_tr, Y_te] = [load_npz(os.path.join(data_path, '{}.npz'.format(name)))\\\n",
    "                            for name in ['X_tr', 'X_te', 'Y_tr', 'Y_te']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15539, 5000), (15539, 5000), (15539, 3993), (15539, 3993))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape, Y_tr.shape, Y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: use SGH to learn a embedding for y\n",
    "y-->z-->y to optimize a embeding $z\\in\\{0,1\\}^{100}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ymean = Y_tr.mean(axis=0).astype('float64')\n",
    "yvar = np.clip(Y_tr.toarray().var(axis=0), 1e-7, np.inf).astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import function\n",
    "#from tensorflow.train import Saver\n",
    "# reduced MRF with stochastic neuron \n",
    "\n",
    "dim_input = Y_tr.shape[1]\n",
    "dim_hidden= 100\n",
    "batch_size = 1500\n",
    "learning_rate = 1e-2\n",
    "max_iter = 10000\n",
    "\n",
    "alpha = 1e-3\n",
    "beta = 1e-3\n",
    "\n",
    "def VAE_stoc_neuron(alpha, batch_size, learning_rate, max_iter):\n",
    "    \n",
    "    g = tf.Graph()\n",
    "    dtype = tf.float32\n",
    "    \n",
    "    with g.as_default():\n",
    "        x = tf.placeholder(dtype, [None, dim_input], name='x')\n",
    "        \n",
    "        # define doubly stochastic neuron with gradient by DeFun\n",
    "        #gradient_func: [x1,...,xn,dL/dy1,dL/dy2,...,dL/dym]---->[dL/dx1,...,dL/dxn]\n",
    "        @function.Defun(dtype, dtype, dtype)\n",
    "        def DoublySNGrad(logits, epsilon, dprev):\n",
    "            prob = 1.0 / (1 + tf.exp(-logits))\n",
    "\n",
    "            # unbiased\n",
    "            dlogits = prob * (1 - prob) * (dprev)\n",
    "            return dlogits, 0.\n",
    "\n",
    "        @function.Defun(dtype, dtype, grad_func=DoublySNGrad)\n",
    "        def DoublySN(logits, epsilon):\n",
    "            prob = 1.0 / (1 + tf.exp(-logits))\n",
    "            yout = (tf.sign(prob - epsilon) + 1.0) / 2.0\n",
    "            return yout\n",
    "        \n",
    "        with tf.name_scope('encode'):\n",
    "            wencode = tf.Variable(tf.random_normal([dim_input, dim_hidden], stddev=1.0 / tf.sqrt(float(dim_input)), dtype=dtype),\n",
    "                                       name='wencode')\n",
    "            bencode = tf.Variable(tf.random_normal([dim_hidden], dtype=dtype), name='bencode')\n",
    "            hencode = tf.matmul(x, wencode) + bencode\n",
    "            # determinastic output\n",
    "            hepsilon = tf.ones(shape=tf.shape(hencode), dtype=dtype, name='hepsilon') * .5\n",
    "            with tf.name_scope('MLE_logistic_regression'):\n",
    "                yout = DoublySN(hencode, hepsilon) #activation\n",
    "            \n",
    "        with tf.name_scope('decode'):\n",
    "            with tf.name_scope('scale'):\n",
    "                scale_para = tf.Variable(tf.constant(yvar, dtype=dtype), name=\"scale_para\")\n",
    "                shift_para = tf.Variable(tf.constant(ymean, dtype=dtype), name=\"shift_para\")\n",
    "            wdecode = tf.Variable(tf.random_normal([dim_hidden, dim_input], stddev=1.0 / tf.sqrt(float(dim_hidden)), dtype=dtype), \n",
    "                                  name='wdecode')\n",
    "            xout = tf.matmul(yout, wdecode) * tf.abs(scale_para) + shift_para\n",
    "        \n",
    "        with tf.name_scope('loss'):\n",
    "            monitor = tf.nn.l2_loss(xout - x, name='l2-loss') \n",
    "            loss = monitor + alpha * tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=hencode, labels=yout, name='cross_entropy'))\\\n",
    "                    + beta * tf.nn.l2_loss(wdecode, name='regulerization')\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "            tf.summary.scalar('l2-monitor',monitor)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        # optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        sess = tf.Session(graph=g)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(tf_board_path+'/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(tf_board_path+'/test', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        train_err = []\n",
    "        for i in xrange(max_iter):\n",
    "            indx = np.random.choice(Y_tr.shape[0], batch_size)\n",
    "            ybatch = Y_tr[indx].toarray()\n",
    "            \n",
    "            if i % 2000 == 0: #record runtime every 2000 step\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #trace runtime\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, monitor_value, loss_value, summary= sess.run([train_op, monitor, loss, merged],\n",
    "                                                                feed_dict={x: ybatch}, \n",
    "                                                                options=run_options,\n",
    "                                                                run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step{}'.format(i))\n",
    "                learning_rate = 0.5 * learning_rate\n",
    "            else: # ordinary training op\n",
    "                _, monitor_value, loss_value, summary= sess.run([train_op, monitor, loss, merged],\n",
    "                                                            feed_dict={x: ybatch})\n",
    "                train_writer.add_summary(summary, i)\n",
    "\n",
    "            if i % 100 == 0: #validate monitor\n",
    "                indx = np.random.choice(Y_te.shape[0], batch_size)\n",
    "                ytest_batch = Y_te[indx].toarray()\n",
    "                summary = sess.run(merged, \n",
    "                                   feed_dict={x:ytest_batch})\n",
    "                #test_writer.add_run_metadata(run_metadata, 'step{}'.format(i))\n",
    "                test_writer.add_summary(summary, i)\n",
    "                \n",
    "\n",
    "    \n",
    "        node_list = ['yout', 'pout', 'xout', 'wencode', 'bencode', 'wdecode', 'scale_para', 'shift_para']\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        para_list = {}\n",
    "        for var in t_vars:\n",
    "            para_list[var.name] = sess.run(var)\n",
    "    \n",
    "    return g, node_list, para_list, train_err\n",
    "\n",
    "time.tic()\n",
    "g, node_list, para_list, train_err = VAE_stoc_neuron(alpha, batch_size, learning_rate, max_iter)\n",
    "time.toc()\n",
    "\n",
    "print ('see infomation by running command tensorboard --logir={}'.format(tf_board_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(para_list, model_path+'/paralist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_list = joblib.load(model_path+'/paralist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# reconstruct the image via the learned codes and templates. It generates the Figure 3. in main text.\n",
    "\n",
    "W = para_list['encode/wencode:0']\n",
    "b = para_list['encode/bencode:0']\n",
    "\n",
    "shift = para_list['decode/scale/shift_para:0']\n",
    "scale = para_list['decode/scale/scale_para:0']\n",
    "\n",
    "U = para_list['decode/wdecode:0']\n",
    "\n",
    "logits = np.dot(Y_tr.toarray(), W) + b\n",
    "epsilon = 0.5 \n",
    "pres = 1.0 / (1 + np.exp(-logits))\n",
    "Z_tr = (np.sign(pres - epsilon) + 1.0) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Random Forest CLF and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=1)\n",
    "time.tic()\n",
    "#clf.fit(X_tr, Z_tr)\n",
    "training_time = time.tocvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(clf, os.path.join(model_path , 'label0.pkl'), compress=3)# only one classifiers, name for convention#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.tic()\n",
    "clf = joblib.load(os.path.join(model_path , 'label0.pkl'))\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss brute force search\n",
    "nn_index = faiss.index_factory(Z_tr.shape[1], \"Flat\", faiss.METRIC_L2)   # build the index\n",
    "time.tic()\n",
    "nn_index.add(Z_tr.astype('float32'))\n",
    "time.toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = para_list['encode/wencode:0']\n",
    "b = para_list['encode/bencode:0']\n",
    "\n",
    "shift = para_list['decode/scale/shift_para:0']\n",
    "scale = para_list['decode/scale/scale_para:0']\n",
    "\n",
    "U = para_list['decode/wdecode:0']\n",
    "\n",
    "Z_te = clf.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = np.dot(Z_te, U)*scale+shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_s = scipy.sparse.csr_matrix(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(truth, vote, k):\n",
    "    '''\n",
    "    evaluate precision at k for a vote vector\n",
    "    p@k = num of correct prediction in topk / k\n",
    "    '''\n",
    "    success = 0\n",
    "    for i in range(truth.shape[0]):\n",
    "        # find the k-largest index using partition selet\n",
    "        # topk are not sorted, np.argsort(vote[topk]) can do that but not needed here\n",
    "        topk = np.argpartition(vote[i], -k)[-k:] \n",
    "        success += np.sum(truth[i, topk])\n",
    "    return success / ((float(truth.shape[0])*k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "for i in np.arange(1,6,2):\n",
    "    print \"p@{} for classification:\\t {}\\n\".format(i, precision_at_k(Y_te, Y_pred, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
